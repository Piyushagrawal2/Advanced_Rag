{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62156a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.docstore.document import Document\n",
    "\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from utils.helper_functions import *\n",
    "from utils.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d45685",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff08ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import replace_t_with_space\n",
    "def encode_pdf_and_get_split_documents(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF file and returns a list of split documents.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the PDF file.\n",
    "        chunk_size (int): Size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of (FAISS vector store, cleaned text documents).\n",
    "    \"\"\"\n",
    "    \n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "\n",
    "    #Load the pdf\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #split the document\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    cleaned_text = replace_t_with_space(split_documents)\n",
    "\n",
    "    #create embadding and vectore store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_documents(cleaned_text, embeddings)\n",
    "\n",
    "    return vector_store, cleaned_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945820be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore, cleaned_text = encode_pdf_and_get_split_documents(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bff0d4",
   "metadata": {},
   "source": [
    "### Create a bm25 index for retrieving documents by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81543ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'there', 'good', 'man!'],\n",
       " ['It', 'is', 'quite', 'windy', 'in', 'London'],\n",
       " ['How', 'is', 'the', 'weather', 'today?']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3386f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.46864736 0.        ]\n",
      "['It is quite windy in London']\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "#Initializing\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Ranking of documents\n",
    "query = \"windy london\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print(scores)\n",
    "\n",
    "doc_top_n = bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "print(doc_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b794fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given documents.\n",
    "\n",
    "    BM25 (Best Matching 25) is a ranking function used in information retrieval.\n",
    "    It's based on the probabilistic retrieval framework and is an improvement over TF-IDF.\n",
    "\n",
    "    Args:\n",
    "    documents (List[Document]): List of documents to index.\n",
    "\n",
    "    Returns:\n",
    "    BM25Okapi: An index that can be used for BM25 scoring.\n",
    "    \"\"\"\n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    # This is a simple approach and could be improved with more sophisticated tokenization\n",
    "    tokenized_docs = [\n",
    "        doc.page_content.split() for doc in documents\n",
    "    ]\n",
    "    return BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2340ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = create_bm25_index(cleaned_text) # Create BM25 index from the cleaned texts (chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8531c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining keyword-based (BM25) and vector-based search.\n",
    "\n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the documents.\n",
    "    bm25 (BM25Okapi): Pre-computed BM25 index.\n",
    "    query (str): The query string.\n",
    "    k (int): The number of documents to retrieve.\n",
    "    alpha (float): The weight for vector search scores (1-alpha will be the weight for BM25 scores).\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: The top k documents based on the combined scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Step 1: Get all documents from the vectorstore\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "\n",
    "    # Step 2: Perform BM25 search\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # Step 3: Perform vector search\n",
    "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
    "    \n",
    "    # Step 4: Normalize scores\n",
    "    vector_scores = np.array([score for _, score in vector_results])\n",
    "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "\n",
    "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) -  np.min(bm25_scores) + epsilon)\n",
    "\n",
    "    # Step 5: Combine scores\n",
    "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores  \n",
    "\n",
    "    # Step 6: Rank documents\n",
    "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "    \n",
    "    # Step 7: Return top k documents\n",
    "    return [all_docs[i] for i in sorted_indices[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ec92f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "for the new market mechanism established in Durban. \n",
      " \n",
      "COP 19 \n",
      "19th session of the Conference \n",
      "of the Parties(COP 19) to \n",
      "the UNFCCC and the 9th session \n",
      "of the CMP 9 to the Kyoto \n",
      "Protocol was held \n",
      " in Warsaw , Poland in 2013 \n",
      "• The Warsaw conference agreed a time plan for countries to table \n",
      "their contributions to reducing or limiting greenhouse gas emissions \n",
      "under the new global climate agreement in 2015.  \n",
      "• It also agreed ways to accelerate efforts to deepen emission cuts over \n",
      "the rest of this decade, and to set up a mechanism to address losses \n",
      "and damage caused by climate change in vulnerable developing \n",
      "countries. \n",
      "• The conference agreed decisions which enhance the implementation \n",
      "of a range of measures already agreed, including climate finance, \n",
      "REDD+, and transparency of reporting on emissions.\n",
      "\n",
      "\n",
      "Context 2:\n",
      "COP 17 \n",
      "17th session of the Conference \n",
      "of the Parties (COP 17) to \n",
      "the UNFCCC and the 7th session \n",
      "of the CMP 7 to the Kyoto \n",
      "Protocol was held in Durban , \n",
      "South Africa in 2011. \n",
      "• The conference led to agreement on a management framework for a \n",
      "future Green Climate Fund . The fund is to distribute US$100bn per \n",
      "year to help poor countries adapt to climate impacts. \n",
      "• The design of the new Green Climate Fund for developing countries \n",
      "was completed through a decision on its governance and other \n",
      "practical arrangements. This opens the way for the Fund to become \n",
      "operational in 2012. \n",
      "• The conference decided that the second commitment period will \n",
      "start on 1 January 2013 and run until 2017 or 2020. The end date and \n",
      "the emission targets for developed countries taking part will be fixed \n",
      "at the UN climate conference to be held at the end of 2012 in Qatar. \n",
      "• The EU confirmed it will participate in the second period of the \n",
      "Protocol beginning in 2013. \n",
      " \n",
      "COP 18\n",
      "\n",
      "\n",
      "Context 3:\n",
      "• Pledges were made by both developed and developing countries \n",
      "prior to and during the COP that took the capitalization of the new \n",
      "Green Climate Fund (GCF) past an initial $10 billion target. \n",
      "• Levels of transparency and confidence-building reached new heights \n",
      "as several industrialized countries submitted themselves to \n",
      "questioning about their emissions targets under a new process called \n",
      "a Multilateral Assessment. \n",
      "• The Lima Ministerial Declaration on Education and Awareness-raising \n",
      "calls on governments to put climate change into school curricula and \n",
      "climate awareness into national development plans. \n",
      "Reducing Emissions from Deforestation and Forest Degradation (REDD) \n",
      "/square4 Reducing Emissions from Deforestation and Forest Degradation (REDD) is a set of steps designed to use \n",
      "market/financial incentives in order to reduce the emissions of greenhouse gases from deforestation \n",
      "and forest degradation.\n",
      "\n",
      "\n",
      "Context 4:\n",
      "2                                                                           \n",
      "www.visionias.in                                                                         ©Vision IAS  \n",
      "/square4 There are very little changes to the annually averaged sunshine; but there can be strong changes in the \n",
      "geographical and seasonal distribution.  \n",
      "/square4 There are three types of orbital variations namely variations in Earth’s eccentricity, changes in the tilt \n",
      "angle of Earth’s axis of rotation and precession of Earth’s axis. Combined together, these produce \n",
      "Milankovitch cycles which have large impact on climate and are notable for their correlation to glacial \n",
      "and interglacial periods. The IPCC finding shows that Milankovitch cycles drove the ice age cycles. \n",
      "Plate  tectonics \n",
      "/square4 Due to temperature variation in the core of the Earth, the mantle plumes and convection currents force \n",
      "the Plates of the Earth to adjust which causes the reconfiguration of the earth Plate. This can affect both\n",
      "\n",
      "\n",
      "Context 5:\n",
      "private sector, or any combination of these. It is being pushed strongly by the World Bank and the UN \n",
      "for setting up the bases for the carbon market and the legal and governance frameworks of countries \n",
      "receiving REDD.  \n",
      "Carbon offsets  are “emissions-saving projects or programmes” that in theory would “compensate” for the \n",
      "polluters’ emissions. The “carbon credits” generated by these projects could then be used by industrialised \n",
      "governments and corporations to meet their targets and/or to be traded within the carbon markets. \n",
      "REDD ++ \n",
      "/square4 “REDD+” goes beyond deforestation and forest degradation, and includes the role of conservation, \n",
      "sustainable management of forests and enhancement of forest carbon stocks. \n",
      "/square4 It is predicted that financial flows for greenhouse gas emission reductions from REDD+ could reach up to \n",
      "US$30 billion a year. This significant North-South flow of funds could reward a meaningful reduction of\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Perform fusion retrieval\n",
    "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.2)\n",
    "docs_content = [doc.page_content for doc in top_docs]\n",
    "show_context(docs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c2ac21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The main topic of the document is related to climate change, specifically discussing the principles and agreements of the UN Conventions, the differentiation between developed and developing countries in addressing climate change, and the importance of greenhouse gases, particularly carbon dioxide. It also mentions the need for mechanisms to address climate change impacts on vulnerable developing countries.\n",
      "\n",
      "Sources:\n",
      "- other provisions of the UN Conventions. \n",
      "/square4 The ‘developing versus developed country’ schism needs to be diluted at the earliest and Developed \n",
      "Countries should avoid watering down the CBDR prin...\n",
      "- the Convention, applicable to all Parties. The ADP is to complete its work as early as possible, but no \n",
      "later than 2015, in order to adopt this protocol, legal instrument or agreed outcome with legal...\n",
      "- the earth’s surface. \n",
      "The main greenhouse gases include: \n",
      "/square4 Water vapour:  It is the most abundant greenhouse gas \n",
      "(GHG), however it spends just a short time in the \n",
      "atmosphere. The amount of w...\n",
      "- should be the basis for differentiation. The developing countries’ need for inclusive growth, sustainable \n",
      "development, poverty eradication, and energy access to all must be recognized as fundamental ...\n",
      "- the rest of this decade, and to set up a mechanism to address losses \n",
      "and damage caused by climate change in vulnerable developing \n",
      "countries. \n",
      "• The conference agreed decisions which enhance the impl...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_classic.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#load pdf\n",
    "loader = PyPDFLoader('data/Climate_change.pdf')\n",
    "document = loader.load()\n",
    "\n",
    "#split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(document)\n",
    "\n",
    "# dense retriever (sementic)\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(splits, embedding)\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "#sparse retriever (keyword - BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(splits)\n",
    "sparse_retriever.k = 3\n",
    "\n",
    "alpha: float = 0.5\n",
    "\n",
    "# Hybrid retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    alpha=alpha,\n",
    "    weights=[alpha, 1 - alpha]\n",
    ")\n",
    "\n",
    "# LLm\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create. RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# Query with parameter\n",
    "query = \"What is the main topic of this document?\"\n",
    "result = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.page_content[:200]}...\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
